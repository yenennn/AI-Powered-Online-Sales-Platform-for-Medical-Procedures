import os
import time
import ast
import re
import numpy as np
import pandas as pd
from dotenv import load_dotenv
import docx
from langchain.text_splitter import RecursiveCharacterTextSplitter
from google import genai
from google.genai import types
from transitions import Machine
from sqlalchemy import create_engine, Column, Integer, String, Float, Text, text
from sqlalchemy.orm import declarative_base, sessionmaker
from pgvector.sqlalchemy import Vector
from sentence_transformers import SentenceTransformer, util

load_dotenv()

# Database configuration
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/vector_db")
Base = declarative_base()


# Define the embedding model
class DocumentEmbedding(Base):
    __tablename__ = 'document_embeddings'

    id = Column(Integer, primary_key=True)
    chunk_id = Column(Integer)
    text = Column(Text)
    embedding = Column(Vector(768))  # Adjust dimension based on your embedding model

    def __repr__(self):
        return f"<DocumentEmbedding(id={self.id}, chunk_id={self.chunk_id})>"


# Create engine and session
engine = create_engine(DATABASE_URL)
Session = sessionmaker(bind=engine)

class LLMClient:
    def __init__(self, api_key=None, model="gemini-2.0-flash-lite"):
        self.api_key = api_key or os.getenv('GEMINI_KEY')
        self.client = genai.Client(api_key=self.api_key)
        self.model = model

    def analyze_sentiment(self, text):
        prompt = (
            "Sen Ã¶zel bir hasta-duygu sÄ±nÄ±flayÄ±cÄ±ysan. "
            "MesajÄ± oku ve yalnÄ±zca positive, neutral veya negative etiketlerinden biriyle yanÄ±t ver. "
            "Pozitif: hasta hevesli, heyecanlÄ± veya hazÄ±r olduÄŸunu ifade ediyorsa; "
            "NÃ¶tr: sadece bilgi amaÃ§lÄ± sorular soruyorsa; "
            "Negatif: Risklerden korktuÄŸunu belli ediyorsa, tereddÃ¼t veya Ã§ekingenlik gÃ¶steriyorsa. "
            f"Mesaj: \"{text}\""
        )
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(max_output_tokens=10, temperature=0.0)
        )
        label = response.text.strip().lower()
        return label if label in ['positive','neutral','negative'] else 'neutral'

    def chat(self, messages, temperature=0.5, max_tokens=600):
        prompt = "\n".join(f"{m['role'].upper()}: {m['content']}" for m in messages)
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(max_output_tokens=max_tokens, temperature=temperature)
        )
        return response.text.strip()

GREETING_PROMPT = """
Sen Dr.{doctor_name}'in {operation_name} operasyonu iÃ§in KARÅILAMA MODÃœLÃœSÃœNSÃœN.
AmaÃ§: Ä°lk olarak hastanÄ±n gerekli Ã¶n bilgilerini toplamalÄ± ve samimi bir ÅŸekilde selamlamalÄ±sÄ±n.

Talimatlar:
1. HastanÄ±n AdÄ± ve SoyadÄ±nÄ± sor.
2. HastanÄ±n YaÅŸÄ±nÄ± sor.
3. Herhangi bir Alerjisi olup olmadÄ±ÄŸÄ±nÄ± Ã¶ÄŸren ğŸ©º.
4. Operasyon iÃ§in beklenen tarih nedir? Sor ğŸ“†.
5. BulaÅŸÄ±cÄ± bir hastalÄ±ÄŸÄ± (Hepatitis B, Hepatitis C, HIV vb.) var mÄ±? Sor ğŸ˜·.
6. Herhangi bir saÄŸlÄ±k problemi veya dÃ¼zenli kullandÄ±ÄŸÄ± ilaÃ§ (HRT, tiroid, diyabet vb.) var mÄ±? Sor.
7. Boy ve kilo bilgisini al.
8. Daha Ã¶nce geÃ§irdiÄŸi ameliyatlar var mÄ±? Sor.

â€“ Her bir cevabÄ± aÃ§Ä±kÃ§a onayla, eksik bilgi kalÄ±rsa tekrar sor.
â€“ SorularÄ± arada boÅŸluk bÄ±rakmadan, akÄ±cÄ± ve empatik bir dille yÃ¶nelt.
â€“ TÃ¼m bilgiler toplandÄ±ktan sonra bu durumu kapat ve bir sonraki duruma geÃ§.
"""

INFO_REQUEST_PROMPT = """
Sen Dr.{doctor_name}'in {operation_name} operasyonu iÃ§in BÄ°LGÄ° MODÃœLÃœSÃœNSÃœN.
AmaÃ§: HastanÄ±n prosedÃ¼rle ilgili sorularÄ±na net, anlaÅŸÄ±lÄ±r ve empatik bir ÅŸekilde cevap vermelisin.

Talimatlar:
1. KullanÄ±cÄ±nÄ±n sorusunu Ã¶zetle ve en gÃ¼ncel tÄ±bbi bilgiyi ver.
2. Gerekirse RAGâ€™den Ã§ekilen pasajlardan kÄ±sa alÄ±ntÄ±lar ekle, ama â€œRAG kullandÄ±mâ€ deme.
3. EÄŸer ihtiyaÃ§ duyduÄŸunuzu dÃ¼ÅŸÃ¼nÃ¼yorsanÄ±z, ikna edici hikaye anlatÄ±mÄ± kullan: BaÅŸarÄ±lÄ± bir {operation_name} vakasÄ±nÄ±n anekdotunu paylaÅŸ. HastanÄ±n sana gÃ¼venmesini saÄŸla."
4. Teknik terimleri sadeleÅŸtir; hasta istemiyorsa karmaÅŸÄ±k jargon kullanma.
5. Her cevabÄ±nÄ±n sonunda hastaya baÅŸka bir isteÄŸi olup olmadÄ±ÄŸÄ±nÄ± farklÄ± ÅŸekillerde sor.
6. EÄŸer kullanÄ±cÄ± fiyatla ilgili bir soru sorarsa, bu oturumun durumunu 'negotiation' olarak gÃ¼ncelle.
"""

NEGOTIATION_PROMPT = """
Sen Dr.{doctor_name}'in {operation_name} operasyonu iÃ§in PAZARLIK MODÃœLÃœSÃœNSÃœN.
AmaÃ§: HastanÄ±n bÃ¼tÃ§esi ve baz fiyat Ã¼zerinden empatik bir pazarlÄ±k yÃ¼rÃ¼tmelisin.

Talimatlar:
1. Ã–nce doktorun baÅŸarÄ± oranÄ±nÄ± ve kalite gÃ¼vencesini vurgula:
   â€œDr. {doctor_name} %98 baÅŸarÄ± oranÄ±na sahipâ€¦â€ gibi.
2. KullanÄ±cÄ±dan bir fiyat teklifi gelmezse bÃ¼tÃ§e aralÄ±ÄŸÄ±nÄ± sor: â€œSizin iÃ§in makul bir fiyat aralÄ±ÄŸÄ± nedir? gibi bir soruylaâ€
3. Gelen teklife gÃ¶re fiyatÄ± ayarla:
   - Operasyonun baz fiyatÄ±: {base_price}
   â€“ Pozitif duygu â†’ teklifi baz al + %5â€“10 artÄ±rÄ±m
   â€“ NÃ¶tr duygu   â†’ baz fiyatÄ± koru
   â€“ Negatif duyguâ†’ teklifi baz al â€“ %5â€“10 indirim
4. KullanÄ±cÄ±nÄ±n teklifi baz fiyatÄ±n %20 altÄ±ndaysa:
   â€œMaalesef bu aralÄ±k mÃ¼mkÃ¼n deÄŸil, lÃ¼tfen biraz yÃ¼kseltinâ€ de.
5. Her adÄ±mda {aggregate_sentiment} deÄŸerini kullanarak stratejini gÃ¼ncelle.
6. PazarlÄ±k tamamlandÄ±ÄŸÄ±nda durumu 'bye' olarak gÃ¼ncelle.
"""

TRANSITION_PROMPT = """
Sen Dr.{doctor_name}'in {operation_name} operasyonu iÃ§in DURUM GEÃ‡Ä°Å KONTROL MODÃœLÃœSÃœNSÃœN.
AmaÃ§: KullanÄ±cÄ±nÄ±n niyetine gÃ¶re durumlar arasÄ± geÃ§iÅŸ kurallarÄ±nÄ± uygula.

Talimatlar:
- Mevcut Durum: {current_state}
- KullanÄ±cÄ± MesajÄ±: {user_text}

OlasÄ± Durumlar: greeting, info_request, negotiation, bye
Kurallar:
 â€¢ KullanÄ±cÄ± â€œmerhabaâ€, â€œselamâ€ gibi ifadeler kullanÄ±rsa â†’ greeting
 â€¢ TÄ±bbi detay veya risk sorusu varsa â†’ info_request
 â€¢ Fiyat, Ã¶deme veya bÃ¼tÃ§e sorusu ise â†’ negotiation
 â€¢ Ã‡Ä±kÄ±ÅŸ yapmak istediÄŸini spesifik olarak belirtirse â†’ bye
 â€¢ HiÃ§biri eÅŸleÅŸmiyorsa â†’ mevcut durumda kal
EÄŸer operasyonla ilgili spesifik bir sorusu kalmadÄ±ÄŸÄ±ndan eminsen negotiation'a yÃ¶nlendir ve fiyat teklifi almaya Ã§alÄ±ÅŸ
YanÄ±t olarak yalnÄ±zca bir kelime ver: greeting, info_request, negotiation veya bye
"""
BYE_PROMPT = """
Sen Dr.{doctor_name}'in {operation_name} operasyonu iÃ§in VEDA MODÃœLÃœSÃœNSÃœN.
AmaÃ§: Sohbeti kibarca sonlandÄ±r.

Talimatlar:
â€“ â€œTeÅŸekkÃ¼rler, yardÄ±mcÄ± olabildiysem ne mutlu. GÃ¶rÃ¼ÅŸmek Ã¼zere.â€ gibi kibar bir veda et.
â€“ Tekrar gelmek isterlerse â€œHer zaman buradayÄ±mâ€ diye ekle.
"""


class Operation:
    def __init__(self, name, base_price):
        self.name = name
        self.base_price = base_price

class Doctor:
    def __init__(self, name, surname, specialty):
        self.name = name
        self.surname = surname
        self.specialty = specialty

STATES = ['greeting','info_request','negotiation','bye']


class ChatSession:
    STATE_PROMPTS = {
        'greeting': GREETING_PROMPT,
        'info_request': INFO_REQUEST_PROMPT,
        'negotiation': NEGOTIATION_PROMPT,
        'transition': TRANSITION_PROMPT,
        'bye': BYE_PROMPT
    }

    def __init__(self, operation, doctor, document_path, patient_risk=False):
        # Initialize the database if needed
        Base.metadata.create_all(engine)

        self.machine = Machine(model=self, states=STATES, initial='greeting')
        for s in STATES:
            self.machine.add_transition(trigger=f'to_{s}', source='*', dest=s)
        self.llm = LLMClient()
        self.operation = operation
        self.doctor = doctor
        self.document_path = document_path
        self.patient_risk = patient_risk
        self.model_embed = "gemini-embedding-exp-03-07"
        self.client_embed = genai.Client(api_key=self.llm.api_key)
        self._generate_embeddings()
        modifier = 1.5 if patient_risk else 1.0
        self.base_price = operation.base_price * modifier
        self.last_offer = None
        self.base_temperature = 0.5
        self.visit_counts = {s: 0 for s in STATES}
        self.chat_history = []
        self.memory_summary = None
        self.state = 'greeting'
        self._add_message('system', self.STATE_PROMPTS['greeting'].format(
            doctor_name=self.doctor.name,
            operation_name=self.operation.name
        ))

    def _extract_text_and_tables_in_order(self):
        doc = docx.Document(self.document_path)
        full_text = []
        for element in doc.element.body:
            if element.tag.endswith('p'):
                para = docx.text.paragraph.Paragraph(element, doc)
                if para.text.strip():
                    full_text.append(para.text)
            elif element.tag.endswith('tbl'):
                table = docx.table.Table(element, doc)
                rows = []
                for row in table.rows:
                    cells = [cell.text.strip() for cell in row.cells]
                    rows.append("\t".join(cells))
                full_text.append("\n".join(rows))
        return "\n".join(full_text)

    def _split_text(self, text, chunk_size=1000, overlap=200):
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
        return splitter.split_text(text)

    def _generate_embedding(self, text):
        response = self.client_embed.models.embed_content(model=self.model_embed, contents=text)
        time.sleep(10)
        return response.embeddings[0].values

    def _generate_embeddings(self):
        """Check if embeddings exist in the database"""
        db_session = Session()

        # Check if we already have embeddings in the database
        count = db_session.query(DocumentEmbedding).count()
        if count == 0:
            db_session.close()
            raise Exception(
                "No embeddings found in database. Please run db_embeddings.py first to generate embeddings."
            )
        else:
            print(f"Found {count} existing embeddings in database.")

        db_session.close()

    def find_best_passage(self, query):
        """Find the most relevant passage using vector similarity search"""
        query_emb = self.client_embed.models.embed_content(model=self.model_embed, contents=query)
        query_vec = query_emb.embeddings[0].values

        db_session = Session()

        # Format the query vector correctly for pgvector
        # It needs to be in the format [x,y,z,...] not {x,y,z,...}
        query_vec_str = str(query_vec).replace('{', '[').replace('}', ']')

        # Use SQLAlchemy's text() function with proper parameter binding
        sql = text("""
            SELECT id, chunk_id, text, embedding <=> :query_vector AS distance
            FROM document_embeddings
            ORDER BY distance ASC
            LIMIT 3
        """).bindparams(query_vector=query_vec_str)

        result = db_session.execute(sql).fetchall()
        db_session.close()

        # Combine the text from the top 3 results
        passages = [row[2] for row in result]  # row[2] is the text column
        return " ".join(passages)

    def _add_message(self, role, content):
        sentiment = self.llm.analyze_sentiment(content)
        self.chat_history.append({'role': role, 'content': content, 'sentiment': sentiment, 'state': self.state})

    def _last_user_message(self):
        for msg in reversed(self.chat_history):
            if msg['role'] == 'user':
                return msg['content']
        return ''

    def _parse_next_state(self, response_text):
        candidate = response_text.strip().lower()
        return candidate if candidate in STATES else self.state

    def _build_prompt(self, user_text):
        transition = self.STATE_PROMPTS['transition'].format(
            doctor_name=self.doctor.name,
            operation_name=self.operation.name,
            current_state=self.state,
            user_text=user_text
        )
        trans_resp = self.llm.chat([
            {'role': 'system', 'content': transition},
            {'role': 'user', 'content': user_text}
        ], temperature=0.0, max_tokens=5)
        next_state = self._parse_next_state(trans_resp)
        if next_state != self.state:
            getattr(self, f'to_{next_state}')()
            self.state = next_state

        # Calculate aggregate sentiment
        sents = [m['sentiment'] for m in self.chat_history if m['role'] == 'user']
        agg_sentiment = {
            'positive': sents.count('positive'),
            'neutral': sents.count('neutral'),
            'negative': sents.count('negative')
        }

        # Format prompt based on state
        if self.state == 'negotiation':
            prompt = self.STATE_PROMPTS[self.state].format(
                doctor_name=self.doctor.name,
                operation_name=self.operation.name,
                base_price=self.base_price,
                aggregate_sentiment=f"positive={agg_sentiment['positive']}, neutral={agg_sentiment['neutral']}, negative={agg_sentiment['negative']}"
            )
        else:
            prompt = self.STATE_PROMPTS[self.state].format(
                doctor_name=self.doctor.name,
                operation_name=self.operation.name
            )

        return prompt, agg_sentiment

    def process_user(self, user_text):
        self._add_message('user', user_text)
        self.visit_counts[self.state] += 1
        temp = min(0.9, self.base_temperature + 0.1 * (self.visit_counts[self.state] - 1))
        if self.state == 'info_request':
            ref = self.find_best_passage(user_text)
        else:
            ref = user_text

        prompt, agg_sentiment = self._build_prompt(user_text)

        if self.state == 'negotiation':
            agg = f"aggregate_sentiment: positive={agg_sentiment['positive']}, neutral={agg_sentiment['neutral']}, negative={agg_sentiment['negative']}"
            system_content = f"{prompt}\n{agg}\nREFERENCE: {ref}"
        else:
            system_content = f"{prompt}\nREFERENCE: {ref}"

        msgs = [{'role': 'system', 'content': system_content}]
        if self.memory_summary:
            msgs.append({'role': 'system', 'content': f"memory_summary: {self.memory_summary}"})
        for m in self.chat_history:
            msgs.append({'role': m['role'], 'content': m['content']})

        reply = self.llm.chat(msgs, temperature=temp)
        self._add_message('assistant', reply)
        return reply

    def debug_embeddings(self):
        """Debug the first embedding in the database"""
        db_session = Session()
        first_doc = db_session.query(DocumentEmbedding).first()
        db_session.close()

        if first_doc:
            print(f"Embedding type: {type(first_doc.embedding)}")
            print(f"Embedding sample: {str(first_doc.embedding)[:100]}...")
        else:
            print("No embeddings found in database")

if __name__ == "__main__":
    load_dotenv()
    operation = Operation("Rhinoplasty", 5000)
    doctor = Doctor("Selcuk", "Coskun", "Plastic Surgeon")
    session = ChatSession(
        operation,
        doctor,
        "Definition and Purpose of Rhinoplasty.docx",
        patient_risk=False
    )
    while session.state != 'bye':
        user_input = input("You: ").strip()
        if not user_input:
            continue
        if user_input.lower() in ("exit", "quit"):
            print("Goodbye!")
            break
        print(session.process_user(user_input))